#!/usr/bin/env python
# encoding: utf-8
#Author - Prateek Mehta


import tweepy #https://github.com/tweepy/tweepy
import json


#Twitter API credentials
consumer_key = "yOn8TiBcEIO6jQGHDoaVXQOy0"
consumer_secret = "jgJU47p5tKTzkLFt9fXhNrGpW4KPfioq0fDi8to1vgjiuZ3eQT"
access_key = "1309951984741412864-Bwa61zLFSTYizyW03plTkG7nxfxSsT"
access_secret = "HVF3588kI4IHvmFSG31VkXMD0y3eYdXOojtnHmTWKOmmF"


def get_all_tweets(screen_name):
    
    #Twitter only allows access to a users most recent 3240 tweets with this method
    
    #authorize twitter, initialize tweepy
    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_key, access_secret)
    api = tweepy.API(auth)
    
    #initialize a list to hold all the tweepy Tweets
    alltweets = []    
    
    #make initial request for most recent tweets (200 is the maximum allowed count)
    new_tweets = api.user_timeline(screen_name = screen_name,count=10)
    
    #save most recent tweets
    alltweets.extend(new_tweets)
    
    #save the id of the oldest tweet less one
    oldest = alltweets[-1].id - 1
    
    #keep grabbing tweets until there are no tweets left to grab
    while len(new_tweets) > 0:
        
        #all subsiquent requests use the max_id param to prevent duplicates
        new_tweets = api.user_timeline(screen_name = screen_name,count=10,max_id=oldest)
        
        #save most recent tweets
        alltweets.extend(new_tweets)
        
        #update the id of the oldest tweet less one
        oldest = alltweets[-1].id - 1
        if(len(alltweets) > 15):
            break
        print ("...%s tweets downloaded so far") % (len(alltweets))
       
    #write tweet objects to JSON
    file = open('tweet.KAlLEAVESM', 'w') 
    print ("Writing tweet objects to JSON please wait...")
    for status in alltweets:
        json.dump(status._json,file,sort_keys = True,indent = 4)
    
    #close the file
    print ("Done")
    file.close()

if __name__ == '__main__':
    #pass in the username of the account you want to download
    get_all_tweets("@KAlLEAVESM")

    from google.cloud import language
from google.cloud.language import enums, types


def analyze_text_entities(text):
    client = language.LanguageServiceClient()
    document = types.Document(
        content=text,
        type=enums.Document.Type.PLAIN_TEXT)

    response = client.analyze_entities(document=document)

    for entity in response.entities:
        print('=' * 79)
        results = [
            ('name', entity.name),
            ('type', enums.Entity.Type(entity.type).name),
            ('salience', entity.salience),
            ('wikipedia_url', entity.metadata.get('wikipedia_url', '-')),
            ('mid', entity.metadata.get('mid', '-')),
        ]
        for k, v in results:
            print('{:15}: {}'.format(k, v))
            
            
// test my text        
text = 'This whole thread is such a mood. I always feel happy when stanning thai star cause their fandom r so chill and fun. But now it's gradually becoming toxic, even more than kpop or cpop. Especialy those unnecessary fanwars and bashing every girl near their bias'
analyze_text_entities(text)

from google.cloud import language
from google.cloud.language import enums, types

def analyze_text_sentiment(text):
client = language.LanguageServiceClient()
document = types.Document(
content=text,
type=enums.Document.Type.PLAIN_TEXT)

response = client.analyze_sentiment(document=document)

sentiment = response.document_sentiment
results = [
    ('text', text),
    ('score', sentiment.score),
    ('magnitude', sentiment.magnitude),
]
for k, v in results:
    print('{:10}: {}'.format(k, v))
//test what I want (text)
text = 'This whole thread is such a mood. I always feel happy when stanning thai star cause their fandom r so chill and fun. But now it's gradually becoming toxic, even more than kpop or cpop. Especialy those unnecessary fanwars and bashing every girl near their bias'
analyze_text_sentiment(text)


